{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-04-11 13:34:01--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Loaded CA certificate '/usr/ssl/certs/ca-bundle.crt'\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: 'input.txt.3'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  4% 7.93M 0s\n",
      "    50K .......... .......... .......... .......... ..........  9% 7.37M 0s\n",
      "   100K .......... .......... .......... .......... .......... 13% 26.2M 0s\n",
      "   150K .......... .......... .......... .......... .......... 18% 22.4M 0s\n",
      "   200K .......... .......... .......... .......... .......... 22% 10.5M 0s\n",
      "   250K .......... .......... .......... .......... .......... 27% 23.9M 0s\n",
      "   300K .......... .......... .......... .......... .......... 32% 19.4M 0s\n",
      "   350K .......... .......... .......... .......... .......... 36% 13.7M 0s\n",
      "   400K .......... .......... .......... .......... .......... 41% 31.0M 0s\n",
      "   450K .......... .......... .......... .......... .......... 45% 65.3M 0s\n",
      "   500K .......... .......... .......... .......... .......... 50%  101M 0s\n",
      "   550K .......... .......... .......... .......... .......... 55% 76.8M 0s\n",
      "   600K .......... .......... .......... .......... .......... 59% 6.93M 0s\n",
      "   650K .......... .......... .......... .......... .......... 64%  102M 0s\n",
      "   700K .......... .......... .......... .......... .......... 68% 35.8M 0s\n",
      "   750K .......... .......... .......... .......... .......... 73% 27.4M 0s\n",
      "   800K .......... .......... .......... .......... .......... 78%  101M 0s\n",
      "   850K .......... .......... .......... .......... .......... 82% 36.2M 0s\n",
      "   900K .......... .......... .......... .......... .......... 87% 29.4M 0s\n",
      "   950K .......... .......... .......... .......... .......... 91%  100M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 96% 66.9M 0s\n",
      "  1050K .......... .......... .......... .........            100%  158M=0.05s\n",
      "\n",
      "2024-04-11 13:34:01 (21.9 MB/s) - 'input.txt.3' saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"features\":[{\"feature_idx\":0,\"name\":\"haiku\",\"type\":{\"dtype\":\"string\",\"_type\":\"Value\"}},{\"feature_idx\":1,\"name\":\"source\",\"type\":{\"dtype\":\"string\",\"_type\":\"Value\"}}],\"rows\":[{\"row_idx\":0,\"row\":{\"haiku\":\"fishing boats\\ncolors of\\nthe rainbow\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":1,\"row\":{\"haiku\":\"ash wednesday--\\ntrying to remember \\nmy dream\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":2,\"row\":{\"haiku\":\"snowy morn--\\npouring another cup\\nof black coffee\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":3,\"row\":{\"haiku\":\"shortest day\\nflames dance\\nin the oven\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":4,\"row\":{\"haiku\":\"haze\\nhalf the horse hidden\\nbehind the house\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":5,\"row\":{\"haiku\":\"low sun\\nthe lady in red\\non high heels\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":6,\"row\":{\"haiku\":\"advent\\nthe passing stranger\\nfarts\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":7,\"row\":{\"haiku\":\"tarn\\na bubble in\\nthe ice\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":8,\"row\":{\"haiku\":\"snowflakes\\nnew asphalt\\nin the holes\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":9,\"row\":{\"haiku\":\"Crystal Night'\\n   gusts of rain\\n      outside\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":10,\"row\":{\"haiku\":\"rain\\nthe sound of a horse galloping\\nthrough leaves\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":11,\"row\":{\"haiku\":\"winter stars\\nsuddenly a whiff\\nof perfume\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":12,\"row\":{\"haiku\":\"hungry\\nhalf of the moon\\nhidden\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":13,\"row\":{\"haiku\":\"rain\\nanother leaf\\ndown\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":14,\"row\":{\"haiku\":\"sharia\\nthe sound of one hand\\nclapping\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":15,\"row\":{\"haiku\":\"the sound of geese\\ndrowned by the sound of the train\\nthis morning\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":16,\"row\":{\"haiku\":\"autumn sun\\nmy shadow over\\ntombstones\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":17,\"row\":{\"haiku\":\"fly fishing;\\nthe sound of the wind\\nin the reel\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":18,\"row\":{\"haiku\":\"december\\na long shadow\\njoins another\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":19,\"row\":{\"haiku\":\"end of path\\nsnowflakes melting\\non the pond\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":20,\"row\":{\"haiku\":\"morning frost\\nshe leaves\\nfirst\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":21,\"row\":{\"haiku\":\"evening walk\\nsmell of tar between\\npines\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":22,\"row\":{\"haiku\":\"dachau\\na blue sky above\\nthe chimneys\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":23,\"row\":{\"haiku\":\"Deep autumn;\\nThe apple colder\\nIn the tree.\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":24,\"row\":{\"haiku\":\"visiting the graves\\nstronger the October wind\\nat my grandparents'\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":25,\"row\":{\"haiku\":\"over the hedge\\na dragonfly\\neast\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":26,\"row\":{\"haiku\":\"a bubble\\nbursts on surface\\nfull moon\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":27,\"row\":{\"haiku\":\"rain\\nthe white lilac\\nlow\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":28,\"row\":{\"haiku\":\"my hand\\n    on her hip\\n         full moon\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":29,\"row\":{\"haiku\":\"three petals fall\\nfrom the purple coneflower...\\nalmost summer\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":30,\"row\":{\"haiku\":\"instant message--\\nmoon reveals more\\nof herself each night\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":31,\"row\":{\"haiku\":\"Out of the well\\nBy the bucket \\nI hunt A moon\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":32,\"row\":{\"haiku\":\"nude beach\\na stranger covers me\\nwith his shadow\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":33,\"row\":{\"haiku\":\"garden wedding --\\nunder the cherry blossoms\\nthe bride's blush deepens\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":34,\"row\":{\"haiku\":\"rainy New York\\nfrom the tenth storey window\\nblack umbrellas bloom\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":35,\"row\":{\"haiku\":\"drunk on the beach-\\nthe moon in my sake cup\\ndisappears\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":36,\"row\":{\"haiku\":\"faintly purple\\nagainst the moon -\\npines in the light\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":37,\"row\":{\"haiku\":\"the last light of day ~\\npurple rhododendrons\\ndissolve in the dark\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":38,\"row\":{\"haiku\":\"wisteria -\\nblooming before\\nthe end of rain\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":39,\"row\":{\"haiku\":\"through lace ~\\nthe tracery of frost\\non glass\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":40,\"row\":{\"haiku\":\"rain\\nfalls from the trees\\non the blue iris\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":41,\"row\":{\"haiku\":\"winter beach -\\nthree grey lines\\n\\\"of sand\",\"source\":\" sea and sky\\\"\"},\"truncated_cells\":[]},{\"row_idx\":42,\"row\":{\"haiku\":\"winter -\\nwhite peonies\\nin falling snow\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":43,\"row\":{\"haiku\":\"long illness –\\npink dogwood blooming\\nwithout me\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":44,\"row\":{\"haiku\":\"lunch al fresco\\nleaving out a chair\\nfor the sun \",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":45,\"row\":{\"haiku\":\"the attention\\nhis dog gets -\\nhomeless man\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":46,\"row\":{\"haiku\":\"remembering a song\\nfrom my childhood\\nplum blossoms\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":47,\"row\":{\"haiku\":\"father’s pills\\nthe palette of\\nautumn leaves\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":48,\"row\":{\"haiku\":\"end of summer\\nour memories\\nin zip files\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":49,\"row\":{\"haiku\":\"waking up\\nwith freckles & curls\\nsummer break\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":50,\"row\":{\"haiku\":\"long night\\non my window—a spider\\nclimbing the moon\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":51,\"row\":{\"haiku\":\"train window\\nthe fingerprints\\nfrom past journeys\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":52,\"row\":{\"haiku\":\"old love letter\\nthe crinkled edges\\nof poppy petals\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":53,\"row\":{\"haiku\":\"his scent gone\\nfrom every room\\nwinter jasmine\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":54,\"row\":{\"haiku\":\"overnight snow\\nhis side of the bed\\nuntouched\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":55,\"row\":{\"haiku\":\"forest trail\\nrunning to the end\\nof my thoughts\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":56,\"row\":{\"haiku\":\"learning to eat\\naround bruises\\nwinter apples\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":57,\"row\":{\"haiku\":\"winter night \\nextending my word \\non scrabble\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":58,\"row\":{\"haiku\":\"thumbing through\\nan old rolodex\\nwinter light\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":59,\"row\":{\"haiku\":\"dandelion field\\nmy voice dissipates\\nin the wind\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":60,\"row\":{\"haiku\":\"trailing behind\\nthe other hikers\\ntaste of dust\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":61,\"row\":{\"haiku\":\"summer break\\nthe sun scatters\\nmy freckles\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":62,\"row\":{\"haiku\":\"summer’s end\\nin a jar of shells\\nthe smell of salt air\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":63,\"row\":{\"haiku\":\"early morning dew\\nsqueezing the last drop\\nfrom my teabag\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":64,\"row\":{\"haiku\":\"spring fever\\ntree roots cracking\\nthe concrete\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":65,\"row\":{\"haiku\":\"tourist season\\nthe zen garden\\nfills up with noise\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":66,\"row\":{\"haiku\":\"unknotting\\nthe phone cord\\nmother’s day\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":67,\"row\":{\"haiku\":\"glowing embers\\nI start my story\\nfrom the end\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":68,\"row\":{\"haiku\":\"office window\\ncomings and goings\\nof butterflies\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":69,\"row\":{\"haiku\":\"beach stroll\\ndipping our feet\\nin the stars\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":70,\"row\":{\"haiku\":\"all day rain\\nlowering the blinds\\nin her dollhouse\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":71,\"row\":{\"haiku\":\"late night stroll\\nthrough parted curtains\\nlives of others\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":72,\"row\":{\"haiku\":\"dusting off\\nthe piano keys\\nautumn wind\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":73,\"row\":{\"haiku\":\"autumn rain\\npuddles fill up\\nwith moonlight\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":74,\"row\":{\"haiku\":\"mountain cave --\\nfrom out of darkness\\nthe morning light\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":75,\"row\":{\"haiku\":\"after its first flight\\nthe young gerfalcon's talons\\ntighter on my glove\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":76,\"row\":{\"haiku\":\"winter roses\\nI am tired of reading\\nbetween the lines\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":77,\"row\":{\"haiku\":\"storm--\\nthe monologue\\nof every tree\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":78,\"row\":{\"haiku\":\"candle snuffer--\\nour eyes adjust\\nto the smoke wisp\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":79,\"row\":{\"haiku\":\"the patter and hiss \\nof gentle raining\\n--cloudlight \",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":80,\"row\":{\"haiku\":\"lull--\\nreading into the braille \\nof your goose bumps\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":81,\"row\":{\"haiku\":\"afternoon malaise--\\nslant light \\nat the oranges bowl\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":82,\"row\":{\"haiku\":\"first day of school--\\nthe house fills with the space \\nbetween second hand ticks\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":83,\"row\":{\"haiku\":\"downpour--\\nthe dead animal smell\\nshifts toward memory\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":84,\"row\":{\"haiku\":\"dusk--\\nthe last whistle of something\\nas I turn on a reading light\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":85,\"row\":{\"haiku\":\"airport window--\\nthe cloud of my breath\\nas your plane reaches the clouds\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":86,\"row\":{\"haiku\":\"midnight moon--\\nafraid\\nto turn another page\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":87,\"row\":{\"haiku\":\"in the no-name vine\\nbluebirds\\nat the red berries\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":88,\"row\":{\"haiku\":\"fresh snow--\\nthe new neighbors\\nwith all the porch lights on\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":89,\"row\":{\"haiku\":\"night shift--\\na quick break\\nto look at the stars...\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":90,\"row\":{\"haiku\":\"freeze warning--\\nlight from the arc welder\\nbrightens the night\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":91,\"row\":{\"haiku\":\"freeze warning--\\nthe need to pee\\nonce we hit traffic\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":92,\"row\":{\"haiku\":\"freeze warning--\\njust enough change\\nfor coffee\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":93,\"row\":{\"haiku\":\"freeze warning---\\nfrom the end of the bar\\nshe looks back\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":94,\"row\":{\"haiku\":\"freeze warning--\\na couple of homeless guys\\nfeeding seagulls\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":95,\"row\":{\"haiku\":\"dust--\\ndistant thunder\\nmoves on\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":96,\"row\":{\"haiku\":\"porn shop--\\neach cock\\nstares at her\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":97,\"row\":{\"haiku\":\"scrabble dictionary--\\nlighting one candle\\nfrom another\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":98,\"row\":{\"haiku\":\"rusting nail--\\nthe coffinmaker\\ntalks of price\",\"source\":\"tempslibres\"},\"truncated_cells\":[]},{\"row_idx\":99,\"row\":{\"haiku\":\"labor day-\\ni dust of\\nmy resume\",\"source\":\"tempslibres\"},\"truncated_cells\":[]}],\"num_rows_total\":144123,\"num_rows_per_page\":100,\"partial\":false}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "curl: (3) URL rejected: Bad hostname\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100 12871  100 12871    0     0   5558      0  0:00:02  0:00:02 --:--:--  5562\n"
     ]
    }
   ],
   "source": [
    "!curl -X GET \\ \"https://datasets-server.huggingface.co/rows?dataset=davanstrien%2FHaiku_Dataset&config=default&split=train&offset=0&length=100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"https://huggingface.co/api/datasets/davanstrien/Haiku_Dataset/parquet/default/train/0.parquet\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    97  100    97    0     0     75      0  0:00:01  0:00:01 --:--:--    75\n",
      "100    97  100    97    0     0     75      0  0:00:01  0:00:01 --:--:--    75\n"
     ]
    }
   ],
   "source": [
    "!curl -X GET \\\n",
    "     \"https://huggingface.co/api/datasets/davanstrien/Haiku_Dataset/parquet/default/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputnew' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43minputnew\u001b[49m\u001b[38;5;241m.\u001b[39mtxt\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputnew' is not defined"
     ]
    }
   ],
   "source": [
    "inputnew.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as file:\n",
    "    content = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset charlen :  1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"dataset charlen : \" , len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#first 1000 characters, includes spaces as well\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "##to note , text is just the collection of characters of the dataset in order of their appearance\n",
    "##if we create a set of (text) we will get all the characters in order of their appearnace but all of them would be unique(characters, not words , so it wont make sense)\n",
    "##if we make a list out of those sets then its content and orientation remains same ,just now it behaves as a list rather than a set\n",
    "##if we sort that list it will display all unique characters of \"text\" in alphabetical order \n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size =len(chars)\n",
    "print((chars))\n",
    "print(\"\".join(chars))#joins all individual chars from list into a single string \n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 63, 1, 52, 39, 51, 43, 1, 47, 57, 1, 63, 39, 58, 46, 39, 56, 58, 46]\n",
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi therz\n"
     ]
    }
   ],
   "source": [
    "##creating a character mapping from characters to integers, basically encodinga and decoding our characters\n",
    "stoi = {ch:i for i , ch in enumerate(chars)}##string to int\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] ##encoding operation, string input encoded int output\n",
    "decode  = lambda l: ''.join([itos[i] for i in l])# decoding operation ,takes encoded int input and decodes it by outputting decoded string\n",
    "print (encode(\"my name is yatharth\"))\n",
    "##any element in encode, in this scenario , cant be greater than 64\n",
    "trylist = [51, 63, 1, 52, 39, 51, 43, 1]\n",
    "trylist2 = [46, 47, 1, 58, 46, 43, 56, 64]\n",
    "print(encode(\"hi there\"))\n",
    "print (decode(trylist2))\n",
    "##the above is an example of character level tokenizer where each encoding element is responsible for change in a character , in general people use sub-word encoding for stronger encoding \n",
    "##the simplicity of this encoder results in a relatively long encoded list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "##now we use our encoder to encode out \"text\"\n",
    "import torch \n",
    "data = torch.tensor(encode(text),dtype = torch.long)\n",
    "print (data.shape , data.dtype)\n",
    "print (data[:1000]) #viewing the 1st 1000 data encodings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now we will split the data into training and validation \n",
    "n = int(0.9*len(data))\n",
    "train_data= data[:n]\n",
    "val_data= data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "tensor([47, 56, 57, 58,  1, 15, 47, 58])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "When input is tensor([18]) the targe : 47\n",
      "When input is tensor([18, 47]) the targe : 56\n",
      "When input is tensor([18, 47, 56]) the targe : 57\n",
      "When input is tensor([18, 47, 56, 57]) the targe : 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the targe : 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the targe : 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the targe : 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the targe : 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data [:block_size+1]\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "print(x)\n",
    "print(y)\n",
    "print(train_data[:block_size+1])\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target=y[t]\n",
    "    print(f\"When input is {context} the targe : {target}\")\n",
    "###here in a sense the block_Size determines the size of the context window , and\n",
    "### x and y determine the context and target pairss, in lay man terms they help machine understand what action to take based on present circumstane \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "---\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 #how many independant sequences will we process in parallel\n",
    "block_size = 8 # what is the context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    #generate a small baatch of data of input x and targets y \n",
    "    data = train_data if split =='train' else val_data\n",
    "    ix = torch.randint (len(data)-block_size, (batch_size,))\n",
    "    ##using torch.stack to make a matrix out of the rows ix\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x ,y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('---')\n",
    "\n",
    "for b in range (batch_size): \n",
    "    for t in range (block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        ####here b,t indexing denotes which block of which batch we are currently iterating \n",
    "        print (f\"when input is {context.tolist()} the target: {target}\")\n",
    "##the 4 by 8 tensor naturally contains 4*8=32 examples, and they are completely independant as far as the transformer is concerned\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8829, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "##now that we have an adequate input we will feed it into a neural network which acts as our model\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "##using the bigram language model , see reference later\n",
    "class mymodel1(nn.Module):\n",
    "    def __init__ (self, vocab_size):\n",
    "        super().__init__()\n",
    "        #each token directly reads off the logits for the next token from a lookup table \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        #idx and targets are both (B,T) tensor of integers \n",
    "        logits = self.token_embedding_table(idx) #(B,T,C)\n",
    "        ## pytorch is going to convert this into a batch(4) by time(8) by channel(vocab_Size=65) tensor\n",
    "        ##it is important to note that we will have to change the dimensions of our logits from B/T/C to B*T/C because this is the format the functional cross entropy expects it to be in \n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "                \n",
    "            B,T,C = logits .shape\n",
    "            #changing dimensions of logits \n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            #if second dim of 2-d matrix not provided in view, it is assumed to remove error possibillity \n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            ##compares quality of logits wrt targets \n",
    "        \n",
    "        return logits, loss \n",
    "        ###in essence we are predicting what comes next based on the single identity of a token\n",
    "        ##a brief understanding of how the model functions so far would be taht we have created an embedding table which is a sqaure matrix of size = vocab_size\n",
    "        ## here each embedding will refer to it when invoked using idx and \"pluck out\" the row that has the same value as itself , i.e. element 24 will pluck out the 24th row of the table \n",
    "        ## now we will create the generate part of the model\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is (B,T) array of indices in the current context \n",
    "        for _ in range(max_new_tokens): \n",
    "            #get_prediction\n",
    "            logits, loss = self(idx)\n",
    "            #focus only on the last time step\n",
    "            logits = logits[:,-1,:] #becomes (B,C)\n",
    "            ##applying softmax on logits to get probabillities \n",
    "            probs =F.softmax(logits,dim=-1)#(B,C)\n",
    "            ##sample from the distribution \n",
    "            ##\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) #(B,1) for each batch we have a single prediction for what comes next \n",
    "            \n",
    "            #append sampled index into the running sequence \n",
    "            idx = torch.cat((idx, idx_next) ,dim=1) #(B,T+1)\n",
    "            ##above we can see that whatever is predicted is concatenated to the prvious idx to continue flow of text generation \n",
    "            \n",
    "        return idx \n",
    "\n",
    "m = mymodel1(vocab_size)\n",
    "logits,loss = m(xb,yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "##the line below decodes the encoded elements on a per batch basis , it will be garbage for now because we havent trained our model yet \n",
    "print(decode(m.generate(idx= torch.zeros((1,1),dtype = torch.long),max_new_tokens=100)[0].tolist()))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0508344173431396\n"
     ]
    }
   ],
   "source": [
    "#creating pytorch optimizing object \n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "##now for the training loop \n",
    "epochs=1000\n",
    "loss_array = []\n",
    "for steps in range (epochs):\n",
    "    xb ,yb = get_batch('train')\n",
    "    #evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_array.append(loss.item())\n",
    "    \n",
    "print(loss.item())\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ARE:\n",
      "\n",
      "THay mee gantive an mputis t cl, wnthyont?\n",
      "Anthecomy, angrrid l ht t mund r fortharet he haraderoy youre.\n",
      "Tan\n",
      "Who ndedyo te Bun:\n",
      "EDWhe f ld t malender, ce s can, qustas, ag r, in nd obllouthouche menonsewim yon hesowetwonese RENostoup Y herin, thes covengharothome areasall fer toitolis avis rd wawakeast gor thaverindavet we y coran,\n",
      "LULIOLORI awispt a amith wito ss w,\n",
      "\n",
      "LA: arne.\n",
      "\n",
      "\n",
      "MIINCHes,\n",
      "\n",
      "He,\n",
      "T:\n",
      "Hath toreat'siss haut t pane.\n",
      "S:\n",
      "\n",
      "RD:\n",
      "Wicanes mer il ougho shifo t,\n",
      "Meed d s t t ad\n",
      "Aspome ha nd thonthoueren,\n",
      "LE:\n",
      "YWind be, stelalea matr aved\n",
      "Fof hthe; higlf yo fee ayat thind myowelllotowiner d clinetherse front, lle?\n",
      "Angr boucancemin bous holdaid t llinchithe nde\n",
      "ARD VON amurds se,\n",
      "S:\n",
      "Anatoreshow g fout on.\n",
      "Thth he, tod nghe,\n",
      "S:\n",
      "INGard loffre.\n",
      "Be?\n",
      "AR:\n",
      "Y:\n",
      "MEESiourl buto athaithe an, d t tt malloan. ff thes he.\n",
      "Fofent exf d rte mise Gompaile mabe aveno tly hethabr dutorl beveso,\n",
      "IONo ge:\n",
      "I'than offlleveat om yoformouno h appordoulore forat VO:\n",
      "Bur h l meeve\n",
      "\n",
      "I'eness s tisprincoit, avi\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx= torch.zeros((1,1),dtype = torch.long),max_new_tokens=1001)[0].tolist()))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##we have completed above the most simple iteration of the model , now with improving model complexity we will see improvement in content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self attention application\n",
    "to improve our output we can say that improving communication between tokens is key , but the communication should make sense\n",
    "the current token should only be able to communicate with the previous tokens and not the future tokens, becase they virtually dont exist yet and we need to predict those tokens \n",
    "easiest way for tokens to talk to each other is to take the average of the past steps , as it will summarize the current token with context to its history \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the concept of self attention was introduced in the gpt paper \"attention is all you need \" \n",
    "it introduced the transformer model for the first time ,which was a novel architecture that was significantly different from RNN which was dominant in that era \n",
    "\n",
    "key innovation of transformers was the use of self-attention mechanism.\n",
    "RNNs process data sequentially , but transformers were able to process data parallelly which significantly brought down the training time and improved the handling of long range dependencies in text \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of self attention can be explained in detail as follows : \n",
    "input to the self attention layer is aset of vectors \n",
    "each vector represents a word or a token in the input sequence \n",
    "\n",
    "these vectors are typically the output of an embedding/encoding layer\n",
    "\n",
    "query , key , value : for each tokens vector , the self attention mechanism creates three different vectors through learnedd linear transformations\n",
    "these vectors are termed as Query(Q ) , Key (K) and Value(V) the transformations for these are seperate for each vector but are teh same across different positions \n",
    "\n",
    "attention scores : to determine the amount of attention eaach element is giving to other element,  the dot product is calculated between the query vector of each position and keey vector of every other position. \n",
    "this results in a score that signifies how much focus to put on other parts of the input sequence when encoding a particular part \n",
    "\n",
    "these scores are then scaled down for stabillity \n",
    "\n",
    "softmax is applied to these scores to obtain weights of the attention \n",
    "\n",
    "weighted sum : the value vectors are then weighted according to these softmax scores . this represents parts of the input sequence that are importatnt to the current token\n",
    "output for each position is the aggregation of these weighted value vectors ,this output then further passes throught further layers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bag of Words (BoW) model is a simple yet influential method in natural language processing and machine learning for text representation. It's used in various tasks like document classification, sentiment analysis, and information retrieval. The fundamental idea of the BoW model is to represent text data as a bag (multiset) of words without considering grammar or word order but maintaining multiplicity.\n",
    "\n",
    "Here's how the Bag of Words model typically works:\n",
    "\n",
    "Vocabulary Creation: First, a vocabulary of all unique words across all documents in the training set is created. Each word in the vocabulary is assigned a unique index.\n",
    "\n",
    "Document Representation: Each document (or text snippet) is then represented as a vector. The length of this vector is equal to the length of the vocabulary.\n",
    "\n",
    "Word Counting: The simplest way to fill in this vector is by counting the occurrences of each word from the vocabulary in the document. Each position in the vector corresponds to a word in the vocabulary, and the value at that position is the count of that word in the document. This method is known as \"Term Frequency\".\n",
    "\n",
    "Alternatives to Simple Counts: Instead of simple word counts, variants like TF-IDF (Term Frequency-Inverse Document Frequency) can be used. TF-IDF weighs the word counts by a measure of how common or rare they are across all documents, which helps in reducing the impact of frequently occurring words that might be less informative.\n",
    "\n",
    "Handling Unseen Words: If a new document contains words not seen in the training set, these words are typically ignored, as the model doesn't have any information on them.\n",
    "\n",
    "Advantages of the Bag of Words model:\n",
    "\n",
    "Simplicity: It's straightforward to understand and implement.\n",
    "Efficiency: Conversion of text to a numerical form that machine learning algorithms can work with is efficient.\n",
    "Effectiveness: For many tasks, especially with sufficient data, it provides a good baseline performance.\n",
    "\n",
    "\n",
    "Limitations:\n",
    "\n",
    "Loss of Context and Order: Since it treats text as a 'bag' (set) of words, the order and structure of the words are lost, which can be critical in understanding linguistic nuances.\n",
    "Sparsity: The resulting vectors are typically high-dimensional and sparse, which can be inefficient for computation.\n",
    "Vocabulary Size: The size of the vocabulary can become very large for large datasets, leading to high memory consumption.\n",
    "Word Sense Disambiguation: It doesn't handle different meanings of the same word (homonyms) well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 #batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C)) ## here bow is added to represent bag of words\n",
    "for b in range (B):\n",
    "    for t in range (T):\n",
    "        xprev = x[b,:t+1] #(t,C)\n",
    "        xbow[b,t] = torch.mean(xprev,0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##to print lower and upper  triangular matrix:\n",
    "print(torch.tril(torch.ones(3,3)))\n",
    "torch.triu(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "_____________________\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "________________________\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n",
      "________________________\n"
     ]
    }
   ],
   "source": [
    "##we can see how xbow is a mean of the terms in x at current index and all previous indices (thats why 1st term of both are same )\n",
    "##the above method was working but was inefficient , we can increase efficiency by matrix multiplication \n",
    "##it can be acheived by seeing an example below : \n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a =a/torch.sum(a,1,keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a@b\n",
    "print('a=')\n",
    "print(a)\n",
    "print(\"_____________________\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"________________________\")\n",
    "print(\"c=\")\n",
    "print(c)\n",
    "print(\"________________________\")\n",
    "##as wecan see using this technique it is possible to find out the historical average that is to be used by the bag of words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1808, -0.0700],\n",
      "         [-0.0894, -0.4926],\n",
      "         [ 0.1490, -0.3199],\n",
      "         [ 0.3504, -0.2238],\n",
      "         [ 0.3525,  0.0545],\n",
      "         [ 0.0688, -0.0396],\n",
      "         [ 0.0927, -0.0682],\n",
      "         [-0.0341,  0.1332]],\n",
      "\n",
      "        [[ 1.3488, -0.1396],\n",
      "         [ 0.8173,  0.4127],\n",
      "         [-0.1342,  0.4395],\n",
      "         [ 0.2711,  0.4774],\n",
      "         [ 0.2421,  0.0694],\n",
      "         [ 0.0084,  0.0020],\n",
      "         [ 0.0712, -0.1128],\n",
      "         [ 0.2527,  0.2149]],\n",
      "\n",
      "        [[-0.6631, -0.2513],\n",
      "         [ 0.1735, -0.0649],\n",
      "         [ 0.1685,  0.3348],\n",
      "         [-0.1621,  0.1765],\n",
      "         [-0.2312, -0.0436],\n",
      "         [-0.1015, -0.2855],\n",
      "         [-0.2593, -0.1630],\n",
      "         [-0.3015, -0.2293]],\n",
      "\n",
      "        [[ 1.6455, -0.8030],\n",
      "         [ 1.4985, -0.5395],\n",
      "         [ 0.4954,  0.3420],\n",
      "         [ 1.0623, -0.1802],\n",
      "         [ 1.1401, -0.4462],\n",
      "         [ 1.0870, -0.4071],\n",
      "         [ 1.0430, -0.1299],\n",
      "         [ 1.1138, -0.1641]]])\n",
      "tensor([[[ 0.1808, -0.0700],\n",
      "         [-0.0894, -0.4926],\n",
      "         [ 0.1490, -0.3199],\n",
      "         [ 0.3504, -0.2238],\n",
      "         [ 0.3525,  0.0545],\n",
      "         [ 0.0688, -0.0396],\n",
      "         [ 0.0927, -0.0682],\n",
      "         [-0.0341,  0.1332]],\n",
      "\n",
      "        [[ 1.3488, -0.1396],\n",
      "         [ 0.8173,  0.4127],\n",
      "         [-0.1342,  0.4395],\n",
      "         [ 0.2711,  0.4774],\n",
      "         [ 0.2421,  0.0694],\n",
      "         [ 0.0084,  0.0020],\n",
      "         [ 0.0712, -0.1128],\n",
      "         [ 0.2527,  0.2149]],\n",
      "\n",
      "        [[-0.6631, -0.2513],\n",
      "         [ 0.1735, -0.0649],\n",
      "         [ 0.1685,  0.3348],\n",
      "         [-0.1621,  0.1765],\n",
      "         [-0.2312, -0.0436],\n",
      "         [-0.1015, -0.2855],\n",
      "         [-0.2593, -0.1630],\n",
      "         [-0.3015, -0.2293]],\n",
      "\n",
      "        [[ 1.6455, -0.8030],\n",
      "         [ 1.4985, -0.5395],\n",
      "         [ 0.4954,  0.3420],\n",
      "         [ 1.0623, -0.1802],\n",
      "         [ 1.1401, -0.4462],\n",
      "         [ 1.0870, -0.4071],\n",
      "         [ 1.0430, -0.1299],\n",
      "         [ 1.1138, -0.1641]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#implementing above method for xbow\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei/wei.sum(1,keepdim=True)\n",
    "xbow2 = wei @ x \n",
    "##to check if xbow= xbow2 ,since xbow2 serves the same purpose but was just created using a more efficient method \n",
    "print(xbow)\n",
    "print(xbow2)\n",
    "torch.allclose(xbow, xbow2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##3rd version\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei,dim=-1)\n",
    "xbow3 = wei@x\n",
    "torch.allclose(xbow2,xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#version 4 : self attention \n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8 ,32 ##batch, time , channels \n",
    "x = torch.randn(B,T,C)\n",
    "##we are now witnessing a single head perform self attention \n",
    "head_size = 16 \n",
    "##AS written earlier we will multiply key and query as their dot product to get an attention score\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size , bias=False)\n",
    "\n",
    "k = key(x)\n",
    "q =query(x)\n",
    "wei = q@k.transpose(-2,-1) #(B,T,16) @ (B,16,T) ----> (B,T,T)\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "\n",
    "wei = wei.masked_fill(tril==0,float('-inf'))\n",
    "##we have to mask the future tokens in order to prevent notes from the present communicating with notes of the future \n",
    "wei = F.softmax(wei, dim=-1)\n",
    "##we dont aggregate the token x , we aggregate the value v\n",
    "v = value(x) \n",
    "out = wei @v \n",
    "out.shape\n",
    "#self attention called self attention cause all 3 attributes that are keys, queries and values all come from the same token x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \n",
    "attention is a communication mechanism. it can be imagined as nodes of a directed graph looking at each other and aggregating information witha  weighted sum from all nodes taht point to them ,with data-dependent weights \n",
    "There is no notion of space , Attention simply acts over a set of vectors . This is why we need to positionally encode tokens\n",
    "Each example across batch dimension is of course processed completely independently and never \"talk\" to each other \n",
    "In an encoder attention block just delete the single line that does masking with tril, allowing all tokens to communicate.This block here is called a \"decoder\" attention block because it has triangular masking , and is usually used in autoregressive settings , like language modelling \n",
    "self attention just means that the keys and values are produced from the same sources as queries. In \"cross-attention\" , the queries still get produced  from x ,but the keys and values  come from some other ,external source (e.g. an encoder module ) \n",
    "\n",
    "scaled attention additional divides wei by 1/sqrt(head_size). this makes it so when input Q,K are unit variance , wei will be unit vairance as well , ans Softmax will stay diffused and not saturate too much . Illustration below \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei= q@k.transpose(-2,-1)*head_size** -0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0449)\n",
      "tensor(1.0700)\n",
      "tensor(1.0918)\n"
     ]
    }
   ],
   "source": [
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1,-0.2,0.3,-0.2,0.5]),dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1,-0.2,0.3,-0.2,0.5])*8,dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying multi-head attention \n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import BaseAgent\n",
    "import random\n",
    "\n",
    "class Agent(BaseAgent):\n",
    "    def __init__(self, id):\n",
    "        super().__init__(id=id)\n",
    "        \n",
    "    def next_move(self, state):\n",
    "        op_id = 1 if self.id == 2 else 2\n",
    "        history = state['history']\n",
    "        itr = state['current_iter']\n",
    "        \n",
    "        # On the first move, cooperate.\n",
    "        if itr == 1:\n",
    "            return 1\n",
    "        \n",
    "        # Check the last two moves of the opponent.\n",
    "        last_move = history[-1][op_id]\n",
    "        second_last_move = history[-2][op_id] if itr > 1 else 1\n",
    "        \n",
    "        # If the opponent has defected in the last two moves, defect as well.\n",
    "        if last_move == -1 and second_last_move == -1:\n",
    "            return -1\n",
    "        \n",
    "        # Otherwise, cooperate.\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[161], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseAgent\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAgent\u001b[39;00m(BaseAgent):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'agent'"
     ]
    }
   ],
   "source": [
    "from agent import BaseAgent\n",
    "import random\n",
    "\n",
    "class Agent(BaseAgent):\n",
    "    def __init__(self, id):\n",
    "        super().__init__(id=id)\n",
    "        \n",
    "    def next_move(self, state):\n",
    "        op_id = 1 if self.id == 2 else 2\n",
    "        itr = state[\"current_iter\"]\n",
    "        \n",
    "        # On the first move, cooperate.\n",
    "        if itr == 1:\n",
    "            return 1\n",
    "        \n",
    "        # Retrieve the history of moves.\n",
    "        history = state[\"history\"]\n",
    "        last_opponent_move = history[-1][op_id]\n",
    "        \n",
    "        # Implement forgiveness to account for potential misinterpretations.\n",
    "        if itr > 2:\n",
    "            second_last_opponent_move = history[-2][op_id]\n",
    "            if last_opponent_move == -1 and second_last_opponent_move == -1:\n",
    "                # Defect if the opponent has defected for the last two rounds.\n",
    "                return -1\n",
    "            else:\n",
    "                # Otherwise, cooperate.\n",
    "                return 1\n",
    "        else:\n",
    "            # If there's no history of the opponent defecting twice, cooperate.\n",
    "            return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
